---
title: Next-generation matrix demo
format: html
---

```{python}
import numpy as np
```

The next generation matrix $K$ has entries $K_{ij}$. Every infection in group $i$ in this generation will give rise to $K_{ij}$ infections in group $j$ in the next generation.

In this example, there are four groups. The core group transmits many infections to themselves and to travelers, who in turn transfer many infections to the core group (but not to themselves). Children and the general public don't transmit very much.

```{python}
# high and low transmission rates
hi = 3.0
lo = 0.5

K = np.array(
    [
        [hi, lo, hi, lo],  # core
        [lo, lo, lo, lo],  # children
        [hi, lo, lo, lo],  # travelers
        [lo, lo, lo, lo],  # general
    ]
)
```

Given a length-4 vector of infections $x$, representing the number of infections in each group in some generation, the next generation will have $Kx$ infections in each group.

By definition, $v$ and $\lambda$ are an eigenvector and eigenvalue of $K$ if $Kv = \lambda v$:

```{python}
def dominant_eigen(X):
    # do the eigenvalue analysis
    e = np.linalg.eig(X)
    # which eigenvalue is the dominant one?
    i = np.argmax(np.abs(e.eigenvalues))
    dominant_eigenvalue = e.eigenvalues[i]
    # get the corresponding eigenvector
    v = e.eigenvectors[:, i]

    # note that this vector is L2-normalized
    # (I need all.equal because there is some machine precision limitation)
    assert np.isclose(np.sqrt(sum(v**2)), 1.0)

    # but we want an L1-normalized vector
    dominant_eigenvector = v / sum(v)

    print("Dominant eigenvalue: ", dominant_eigenvalue)
    print("Dominant eigenvector:", dominant_eigenvector)


dominant_eigen(K)
```

Note that the Jacobian has the same eigenvectors and an eigenvalue less by unity:

```{python}
J = K - np.identity(4)
dominant_eigen(J)
```

Let's do an example, to show how these would play out. Start with a single infection, in the core group:

```{python}
x0 = np.array([1.0, 0.0, 0.0, 0.0])
```

Then iteratively generate next generations:

```{python}
n_generations = 10

x = x0

results = [x0]

for i in range(n_generations - 1):
  x = np.matmul(K, x)
  results.append(x)
```

Toward the end of the simulation, $x$ will be large, but it has the same distribution as the L1-normalized dominant eigenvector:

```{python}
print("Final numbers of infections: ", results[-1])

print("Same thing, L1-normalized:", results[-1] / sum(results[-1]))
```

Note that this is the same as the L1-normalized dominant eigenvector.

We can also check how the number of infections changes from generation to generation:

```{python}
# get the total number of infections per generation
total_infections = [float(sum(x)) for x in results]
print("Total infections per generation:", total_infections)

# and the ratio of infections between generations
empirical_r = np.exp(np.diff(np.log(total_infections)))

print("Ratio of infections between generations: ", empirical_r)
```

Note that this stabilizes to the dominant eigenvalue.
